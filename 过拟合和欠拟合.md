# 过拟合和欠拟合

## 过拟合：

模型在训练集上表现非常好，而在测试集、验证集和新数据上表现很差，呈现出一种“高方差”状态：训练集误差小，测试集误差远大于训练集，训练误差和测试误差之间差距过大

## 过拟合的原因：

**模型的容量（模型复杂度）与数据集的大小不匹配**

模型的容量是指拟合各种函数的能力，如果模型容量过于大，将不适用于测试集的训练集特质也学会了，就会导致模型的泛化性能变差了

另一方面是数据集规模太小，使得模型过度的挖掘一些不具有代表性的特征，例如模型里有一个叶子图片，具有锯齿状，而模型学习了该图片后都认为叶子应该具有锯齿状边缘

## 过拟合的解决方法：

### 获得更多的训练数据

获得更多的训练数据是最简单的解决过拟合的方法，由于模型的容量和训练数据不匹配，因此增加训练数据，则能够让模型学习到更多的特征，减少噪声的影响，

增加训练数据可以采用数据增强的方法，通过平移旋转等传统图像处理方法，或者Mixup和Cutup等方法来扩充数据

### 降低模型的容量

如果是深度神经网络，就减少一些网络层数，如果是机器学习模型中的GBDT，就控制树的深度，采用剪枝

### 正则化

L1和L2正则化，向目标函数增加了额外的惩罚项来对参数值进行约束，设计为偏好简单模型，以便提高泛化能力

Dropout方法

提前终止，当训练大模型的时候，经常会观察到训练误差和测试误差会慢慢减小，但是到了一个时间点后测试误差又开始上升或者不降低。

这时，我们可以设定当几个epoch内，测试误差都没有一定数量的下降后，就将训练停止

Bagging和其他集成方法

## 欠拟合：

一方面是模型的容量较低，很难拟合训练集，另一方面是训练集的特征提取不好，提取不到具有代表性的特征

## 欠拟合的原因：

模型复杂度不够，就像用线性模型去拟合非线性的数据，就会导致欠拟合

## 欠拟合的解决方法：

提高模型复杂度，线性模型增加高次项改为非线性模型、在神经网络模型中增加网络层数或者神经元个数、深度学习中改为使用参数量更多更先进的模型等等

增加新特征，比如需要用颜色和形状来判断，只输入颜色特征，模型拟合的能力肯定不好
