# 激活函数


## 激活函数的作用是什么？

激活函数的主要作用是在神经网络中引入**非线性因素，提高模型拟合能力。**

如果没有激活函数，神经网络的每一层输出都是对前面输入的线性变化，无法拟合任意ha

## 常见的三种激活函数


|          |                           sigmoid                            |                          tanh                          |                             ReLU                             |
| :------: | :----------------------------------------------------------: | :----------------------------------------------------: | :----------------------------------------------------------: |
|   公式   |                  $f(x)=\frac{1}{1+e^{-x}}$                   |          $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$          |                       $f(x)=max(0,x)$                        |
|   导数   |                     $f'(x)=f(x)(1-f(x))$                     |                    $f'(x)=1-f^2(x)$                    |       $f'(x)=\begin{cases}1,x>0\\0,x\leq0\end{cases}$        |
| 梯度消失 |                           容易造成                           |               也容易造成，但优于sigmoid                |                     可以减缓，优于前两者                     |
| 常见应用 |                          二分类任务                          |                      **RNN网络**                       |                           CNN网络                            |
|   优点   |                      函数平滑，容易求导                      |        ①函数平滑，容易求导<br>②输出关于零点对称        | ①求导更快，收敛更快   <br>②有效缓解了梯度消失问题<br>③增加网络的稀疏性 |
|   缺点   | ①容易造成梯度消失       <br>②存在幂运算，计算量大<br>③其输出不关于零点对称 |     ①容易造成梯度消失  <br>②同样存在计算量大的问题     |                    容易造成神经元的“死亡”                    |

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2017c018d7a7ed4860a9e4cff444aeba77/Untitled.png)

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2017c018d7a7ed4860a9e4cff444aeba77/Untitled%201.png)

![Untitled](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2017c018d7a7ed4860a9e4cff444aeba77/Untitled%202.png)

## ****相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？****

作为激活函数，两者都存在着两端梯度弥散，计算量大的问题。

但是sigmoid是一个非中心对称的函数，输出总是正数，意味着神经网络的隐藏层的每个结点输出都是正数，当在梯度下降时，隐藏层每个权重的更新梯度方向都取决于一个值，要么为全为正，要么全为负，导致收敛速度变慢

此外，sigmoid函数的输出均大于0，作为下一层神经元的输入会导致均值不为0，随着网络的加深，可能会使原始数据的数据分布发生变化，深度神经网络经常要应用bn层，讲数据处理为0均值分布的情况，因此sigmoid不如tanh

## 如何解决ReLU神经元死亡的问题？

1. 采用LeakyReLU等激活函数
2. 设置较小的学习率
3. 采用动态调整学习率算法
